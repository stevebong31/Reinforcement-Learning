{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import Dense, Input \n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam, Adadelta\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HW-steve\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')   #get environment agent from gym\n",
    "env._max_episode_steps = 10001   #change maximum steps. limited to 200 steps\n",
    "\n",
    "input_size = env.observation_space.shape[0]   #state array size\n",
    "output_size = env.action_space.n   #action array size\n",
    "h_size = 20   #hidden layers size\n",
    "\n",
    "dis = 0.95   #discount factor\n",
    "REPLAY_MEMORY = 50000   #buffer data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:   #create class DQN using keras\n",
    "    def __init__(self, input_size, h_size, output_size, name=\"main\"):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.h_size = h_size\n",
    "        self.net_name = name\n",
    "        self._build_network()\n",
    "\n",
    "    def _build_network(self, l_rate=0.1):\n",
    "        self._X = Input(shape = (self.input_size,))    #input \n",
    "        l1 = Dense(self.h_size, activation='tanh')(self._X )   #hidden layer 1\n",
    "        l2 = Dense(self.h_size, activation='tanh')(l1)\n",
    "        self._Qpred = Dense(self.output_size)(l2)   #output\n",
    "        \n",
    "        self.model = Model(self._X, self._Qpred)   #create NN model\n",
    "        self.model.compile(optimizer = Adadelta(lr = l_rate), loss = 'mse')   #model compile\n",
    "\n",
    "    def predict(self, state):\n",
    "        x = np.reshape(state, [1, self.input_size])\n",
    "        return self.model.predict(x)\n",
    "\n",
    "    def update(self, x_stack, y_stack):\n",
    "        return self.model.train_on_batch(x_stack,y_stack)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_train(mainDQN, targetDQN, train_batch):   #batch train using replay random memoey\n",
    "    x_stack = np.empty(0).reshape(0, input_size)    #initailze stack data to 0 \n",
    "    y_stack = np.empty(0).reshape(0, output_size)\n",
    "\n",
    "    for state, action, reward, next_state, done in train_batch:\n",
    "        Q = mainDQN.predict(state)   #predict action from mainDQN\n",
    "\n",
    "        if done:  # terminal\n",
    "            Q[0, action] = reward\n",
    "        else:\n",
    "            # get target from targetDQN\n",
    "            Q[0, action] = reward + dis * np.max(targetDQN.predict(next_state))\n",
    "\n",
    "        y_stack = np.vstack([y_stack, Q])   #stack output\n",
    "        x_stack = np.vstack([x_stack, state])   #stack input\n",
    "\n",
    "    return mainDQN.update(x_stack, y_stack)     # Train network predicted Q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_play(mainDQN):  #test network\n",
    "    s = env.reset()   \n",
    "    reward_sum = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        a = np.argmax(mainDQN.predict(s))\n",
    "        s, reward, done, _ = env.step(a)\n",
    "        reward_sum += reward\n",
    "        if done:\n",
    "            print (\"Total score {}\". format(reward_sum))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    max_episodes = 5000\n",
    "    replay_buffer = deque()\n",
    "    \n",
    "    mainDQN = DQN(input_size, h_size, output_size)   #create main DQN\n",
    "    targetDQN = DQN(input_size, h_size, output_size)   #create target DQN\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        e = 1./((episode / 10) +1)   #epsilon greedy\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "            if np.random.rand(1) < e:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(mainDQN.predict(state))\n",
    "                \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                reward = -500\n",
    "                \n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "            if len(replay_buffer) > REPLAY_MEMORY:\n",
    "                replay_buffer.popleft()\n",
    "            \n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "            if reward > 500:\n",
    "                break\n",
    "                \n",
    "        print (\"Episode: {} steps {}\".format(episode, step_count))\n",
    "        if step_count > 1000:\n",
    "            pass\n",
    "        bot_play(mainDQN)\n",
    "        if episode % 10 == 1 :\n",
    "            for _ in range(50):\n",
    "                minibatch = random.sample(replay_buffer, 10)\n",
    "                loss = replay_train(mainDQN, targetDQN, minibatch)\n",
    "            print (\"Loss :\" , loss)\n",
    "            targetDQN.model.set_weights(mainDQN.model.get_weights())                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 steps 22\n",
      "Total score 10.0\n",
      "Episode: 1 steps 25\n",
      "Total score 13.0\n",
      "Loss : 0.8872568\n",
      "Episode: 2 steps 13\n",
      "Total score 9.0\n",
      "Episode: 3 steps 17\n",
      "Total score 12.0\n",
      "Episode: 4 steps 17\n",
      "Total score 12.0\n",
      "Episode: 5 steps 13\n",
      "Total score 12.0\n",
      "Episode: 6 steps 11\n",
      "Total score 10.0\n",
      "Episode: 7 steps 16\n",
      "Total score 10.0\n",
      "Episode: 8 steps 11\n",
      "Total score 11.0\n",
      "Episode: 9 steps 20\n",
      "Total score 10.0\n",
      "Episode: 10 steps 16\n",
      "Total score 12.0\n",
      "Episode: 11 steps 11\n",
      "Total score 13.0\n",
      "Loss : 0.7092656\n",
      "Episode: 12 steps 12\n",
      "Total score 22.0\n",
      "Episode: 13 steps 31\n",
      "Total score 19.0\n",
      "Episode: 14 steps 14\n",
      "Total score 16.0\n",
      "Episode: 15 steps 12\n",
      "Total score 23.0\n",
      "Episode: 16 steps 20\n",
      "Total score 24.0\n",
      "Episode: 17 steps 21\n",
      "Total score 20.0\n",
      "Episode: 18 steps 19\n",
      "Total score 14.0\n",
      "Episode: 19 steps 23\n",
      "Total score 20.0\n",
      "Episode: 20 steps 16\n",
      "Total score 16.0\n",
      "Episode: 21 steps 15\n",
      "Total score 26.0\n",
      "Loss : 24953.396\n",
      "Episode: 22 steps 16\n",
      "Total score 16.0\n",
      "Episode: 23 steps 21\n",
      "Total score 27.0\n",
      "Episode: 24 steps 15\n",
      "Total score 14.0\n",
      "Episode: 25 steps 16\n",
      "Total score 13.0\n",
      "Episode: 26 steps 13\n",
      "Total score 10.0\n",
      "Episode: 27 steps 12\n",
      "Total score 25.0\n",
      "Episode: 28 steps 24\n",
      "Total score 22.0\n",
      "Episode: 29 steps 17\n",
      "Total score 14.0\n",
      "Episode: 30 steps 19\n",
      "Total score 18.0\n",
      "Episode: 31 steps 14\n",
      "Total score 13.0\n",
      "Loss : 24947.746\n",
      "Episode: 32 steps 28\n",
      "Total score 17.0\n",
      "Episode: 33 steps 19\n",
      "Total score 15.0\n",
      "Episode: 34 steps 12\n",
      "Total score 18.0\n",
      "Episode: 35 steps 37\n",
      "Total score 32.0\n",
      "Episode: 36 steps 26\n",
      "Total score 16.0\n",
      "Episode: 37 steps 23\n",
      "Total score 23.0\n",
      "Episode: 38 steps 23\n",
      "Total score 11.0\n",
      "Episode: 39 steps 29\n",
      "Total score 18.0\n",
      "Episode: 40 steps 18\n",
      "Total score 21.0\n",
      "Episode: 41 steps 11\n",
      "Total score 20.0\n",
      "Loss : 0.5154992\n",
      "Episode: 42 steps 16\n",
      "Total score 11.0\n",
      "Episode: 43 steps 24\n",
      "Total score 13.0\n",
      "Episode: 44 steps 24\n",
      "Total score 17.0\n",
      "Episode: 45 steps 17\n",
      "Total score 29.0\n",
      "Episode: 46 steps 15\n",
      "Total score 13.0\n",
      "Episode: 47 steps 13\n",
      "Total score 17.0\n",
      "Episode: 48 steps 30\n",
      "Total score 15.0\n",
      "Episode: 49 steps 16\n",
      "Total score 14.0\n",
      "Episode: 50 steps 16\n",
      "Total score 31.0\n",
      "Episode: 51 steps 15\n",
      "Total score 18.0\n",
      "Loss : 0.54675394\n",
      "Episode: 52 steps 13\n",
      "Total score 20.0\n",
      "Episode: 53 steps 15\n",
      "Total score 18.0\n",
      "Episode: 54 steps 21\n",
      "Total score 18.0\n",
      "Episode: 55 steps 13\n",
      "Total score 16.0\n",
      "Episode: 56 steps 18\n",
      "Total score 16.0\n",
      "Episode: 57 steps 18\n",
      "Total score 15.0\n",
      "Episode: 58 steps 21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-4a98d43b7ff8>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mbot_play\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-019f5d680158>\u001b[0m in \u001b[0;36mbot_play\u001b[1;34m(mainDQN)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mreward_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mglClearColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyglet\\gl\\lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mGLException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No GL context; create a Window first'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gl_begin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglGetError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgluErrorString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainDQN = DQN(input_size, h_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
