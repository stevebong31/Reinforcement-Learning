{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\HW-steve\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import Dense, Input, Reshape\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "import gym\n",
    "\n",
    "env = gym.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 210, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 100800)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 210)               21168210  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 844       \n",
      "=================================================================\n",
      "Total params: 21,169,054\n",
      "Trainable params: 21,169,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Constants defining our neural network\n",
    "learning_rate = 1e-1\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "\n",
    "\n",
    "X = Input(shape=env.observation_space.shape)\n",
    "h = Reshape((100800,))(X)\n",
    "h = Dense(input_size, activation='relu')(h)\n",
    "Qpred = Dense(output_size, activation='sigmoid')(h)\n",
    "\n",
    "model =Model(X,Qpred)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0  steps: 260 score: 0.0\n",
      "Episode: 1  steps: 232 score: 0.0\n",
      "Episode: 2  steps: 249 score: 0.0\n",
      "Episode: 3  steps: 174 score: 0.0\n",
      "Episode: 4  steps: 274 score: 0.0\n",
      "Episode: 5  steps: 298 score: 0.0\n",
      "Episode: 6  steps: 235 score: 0.0\n",
      "Episode: 7  steps: 175 score: 0.0\n",
      "Episode: 8  steps: 184 score: 0.0\n",
      "Episode: 9  steps: 227 score: 0.0\n",
      "Episode: 10  steps: 195 score: 0.0\n",
      "Episode: 11  steps: 235 score: 0.0\n",
      "Episode: 12  steps: 176 score: 0.0\n",
      "Episode: 13  steps: 196 score: 0.0\n",
      "Episode: 14  steps: 220 score: 0.0\n",
      "Episode: 15  steps: 180 score: 0.0\n",
      "Episode: 16  steps: 251 score: 0.0\n",
      "Episode: 17  steps: 250 score: 0.0\n",
      "Episode: 18  steps: 318 score: 0.0\n",
      "Episode: 19  steps: 343 score: 0.0\n",
      "Episode: 20  steps: 193 score: 0.0\n",
      "Episode: 21  steps: 311 score: 0.0\n",
      "Episode: 22  steps: 212 score: 0.0\n",
      "Episode: 23  steps: 241 score: 0.0\n",
      "Episode: 24  steps: 339 score: 0.0\n",
      "Episode: 25  steps: 187 score: 0.0\n",
      "Episode: 26  steps: 459 score: 0.0\n",
      "Episode: 27  steps: 326 score: 0.0\n",
      "Episode: 28  steps: 412 score: 0.0\n",
      "Episode: 29  steps: 312 score: 0.0\n",
      "Episode: 30  steps: 334 score: 0.0\n",
      "Episode: 31  steps: 311 score: 0.0\n",
      "Episode: 32  steps: 294 score: 0.0\n",
      "Episode: 33  steps: 310 score: 0.0\n",
      "Episode: 34  steps: 327 score: 0.0\n",
      "Episode: 35  steps: 369 score: 0.0\n",
      "Episode: 36  steps: 238 score: 0.0\n",
      "Episode: 37  steps: 457 score: 0.0\n",
      "Episode: 38  steps: 348 score: 0.0\n",
      "Episode: 39  steps: 461 score: 0.0\n",
      "Episode: 40  steps: 260 score: 0.0\n",
      "Episode: 41  steps: 230 score: 0.0\n",
      "Episode: 42  steps: 221 score: 0.0\n",
      "Episode: 43  steps: 292 score: 0.0\n",
      "Episode: 44  steps: 252 score: 0.0\n",
      "Episode: 45  steps: 233 score: 0.0\n",
      "Episode: 46  steps: 349 score: 0.0\n",
      "Episode: 47  steps: 439 score: 0.0\n",
      "Episode: 48  steps: 379 score: 0.0\n",
      "Episode: 49  steps: 362 score: 0.0\n",
      "Episode: 50  steps: 573 score: 0.0\n",
      "Episode: 51  steps: 277 score: 0.0\n",
      "Episode: 52  steps: 316 score: 0.0\n",
      "Episode: 53  steps: 588 score: 0.0\n",
      "Episode: 54  steps: 298 score: 0.0\n",
      "Episode: 55  steps: 298 score: 0.0\n",
      "Episode: 56  steps: 429 score: 0.0\n",
      "Episode: 57  steps: 385 score: 0.0\n",
      "Episode: 58  steps: 269 score: 0.0\n",
      "Episode: 59  steps: 342 score: 0.0\n",
      "Episode: 60  steps: 262 score: 0.0\n",
      "Episode: 61  steps: 327 score: 0.0\n",
      "Episode: 62  steps: 292 score: 0.0\n",
      "Episode: 63  steps: 280 score: 0.0\n",
      "Episode: 64  steps: 218 score: 0.0\n",
      "Episode: 65  steps: 548 score: 0.0\n",
      "Episode: 66  steps: 324 score: 0.0\n",
      "Episode: 67  steps: 362 score: 0.0\n",
      "Episode: 68  steps: 517 score: 0.0\n",
      "Episode: 69  steps: 245 score: 0.0\n",
      "Episode: 70  steps: 199 score: 0.0\n",
      "Episode: 71  steps: 301 score: 0.0\n",
      "Episode: 72  steps: 417 score: 0.0\n",
      "Episode: 73  steps: 309 score: 0.0\n",
      "Episode: 74  steps: 414 score: 0.0\n",
      "Episode: 75  steps: 301 score: 0.0\n",
      "Episode: 76  steps: 366 score: 0.0\n",
      "Episode: 77  steps: 441 score: 0.0\n",
      "Episode: 78  steps: 365 score: 0.0\n",
      "Episode: 79  steps: 487 score: 0.0\n",
      "Episode: 80  steps: 337 score: 0.0\n",
      "Episode: 81  steps: 385 score: 0.0\n",
      "Episode: 82  steps: 348 score: 0.0\n",
      "Episode: 83  steps: 611 score: 0.0\n",
      "Episode: 84  steps: 250 score: 0.0\n",
      "Episode: 85  steps: 535 score: 0.0\n",
      "Episode: 86  steps: 355 score: 0.0\n",
      "Episode: 87  steps: 580 score: 0.0\n",
      "Episode: 88  steps: 408 score: 0.0\n",
      "Episode: 89  steps: 478 score: 0.0\n",
      "Episode: 90  steps: 421 score: 0.0\n",
      "Episode: 91  steps: 409 score: 0.0\n",
      "Episode: 92  steps: 388 score: 0.0\n",
      "Episode: 93  steps: 327 score: 0.0\n",
      "Episode: 94  steps: 771 score: 0.0\n",
      "Episode: 95  steps: 684 score: 0.0\n",
      "Episode: 96  steps: 315 score: 0.0\n",
      "Episode: 97  steps: 571 score: 0.0\n",
      "Episode: 98  steps: 292 score: 0.0\n",
      "Episode: 99  steps: 493 score: 0.0\n",
      "Episode: 100  steps: 496 score: 0.0\n",
      "Episode: 101  steps: 340 score: 0.0\n",
      "Episode: 102  steps: 374 score: 0.0\n",
      "Episode: 103  steps: 327 score: 0.0\n",
      "Episode: 104  steps: 659 score: 0.0\n",
      "Episode: 105  steps: 389 score: 0.0\n",
      "Episode: 106  steps: 609 score: 0.0\n",
      "Episode: 107  steps: 328 score: 0.0\n",
      "Episode: 108  steps: 341 score: 0.0\n",
      "Episode: 109  steps: 931 score: 0.0\n",
      "Episode: 110  steps: 490 score: 0.0\n",
      "Episode: 111  steps: 469 score: 0.0\n",
      "Episode: 112  steps: 481 score: 0.0\n",
      "Total score: 0.0\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = Adam(lr = learning_rate), loss ='mse')\n",
    "\n",
    "max_episodes = 500\n",
    "dis = 0.9\n",
    "step_history = []\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    e = 1. / ((episode / 10) + 1)\n",
    "    step_count = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # The Q-Network training\n",
    "    while not done:\n",
    "        step_count += 1\n",
    "        x = np.expand_dims(state, 0)\n",
    "        # Choose an action by greedily (with e chance of random action) from\n",
    "        # the Q-network\n",
    "        Q = model.predict(x)\n",
    "        \n",
    "        if np.random.rand(1) < e:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q)\n",
    "\n",
    "        # Get new state and reward from environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            Q[0, action] = -100\n",
    "        else:\n",
    "            x_next = np.expand_dims(state, 0)\n",
    "            # Obtain the Q' values by feeding the new state through our network\n",
    "            Q_next =model.predict(x_next)\n",
    "            Q[0, action] = reward + dis * np.max(Q_next)\n",
    "\n",
    "        # Train our network using target and predicted Q values on each episode\n",
    "        model.fit(x,Q,verbose = 0)\n",
    "        state = next_state\n",
    "\n",
    "    step_history.append(step_count)\n",
    "    print(\"Episode: {}  steps: {} score: {}\".format(episode, step_count, reward))\n",
    "    # If last 10's avg steps are 500, it's good enough\n",
    "    if len(step_history) > 10 and np.mean(step_history[-10:]) > 500:\n",
    "        break\n",
    "\n",
    "# See our trained network in action\n",
    "observation = env.reset()\n",
    "reward_sum = 0\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    x =  np.expand_dims(observation, 0)\n",
    "    Q = model.predict(x)\n",
    "    action = np.argmax(Q)\n",
    "\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    #print(\"score: {}\".format(reward))\n",
    "    if done:\n",
    "        print(\"Total score: {}\".format(reward_sum))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
