{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "x = np.random.normal(0.0, 0.55, (10000, 1))\n",
    "y = x * 0.1 + 0.3 + np.random.normal(0.0, 0.03, (10000,1))\n",
    "                     \n",
    "plt.plot(x, y, 'r.')\n",
    "plt.show()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "h = X*W+b\n",
    "\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "Loss = tf.reduce_mean(tf.square(h - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(Loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    _, t_loss = sess.run([optimizer, Loss], feed_dict={X:x, Y:y})\n",
    "    \n",
    "    print(\"Epoch : \", epoch, \" Loss : \", t_loss)\n",
    "    \n",
    "    if epoch ==0 :\n",
    "        y_pred = sess.run(h, feed_dict={X:x})\n",
    "        plt.plot(x, y, 'r.')\n",
    "        plt.plot(x, y_pred, 'b.')\n",
    "        plt.show()\n",
    "    elif (epoch+1) % 100 == 0 :\n",
    "        y_pred = sess.run(h, feed_dict={X:x})\n",
    "        plt.plot(x, y, 'r.')\n",
    "        plt.plot(x, y_pred, 'b.')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "x = np.random.normal(0.0, 0.55, (10000, 1))\n",
    "y = x * 0.1 + 0.3 + np.random.normal(0.0, 0.03, (10000,1))\n",
    "                     \n",
    "plt.plot(x, y, 'r.')\n",
    "plt.show()\n",
    "\n",
    "_input_shape=Input(shape=(1,))\n",
    "h = Dense(1)(_input_shape)\n",
    "\n",
    "model = Model(_input_shape, h)\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='mse')\n",
    "\n",
    "history = model.fit(x, y, epochs=500, batch_size=10000)\n",
    "\n",
    "pred = model.predict(x)\n",
    "plt.plot(x, y, 'r.')\n",
    "plt.plot(x, pred, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Constants defining our neural network\n",
    "learning_rate = 1e-1\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "\n",
    "\n",
    "X = Input(shape=(input_size,))\n",
    "h = Dense(input_size)(X)\n",
    "Qpred = Dense(output_size)(h)\n",
    "\n",
    "model =Model(X,Qpred)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = Adam(lr = learning_rate), loss ='mse')\n",
    "\n",
    "max_episodes = 1000\n",
    "dis = 0.9\n",
    "step_history = []\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    e = 1. / ((episode / 10) + 1)\n",
    "    step_count = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # The Q-Network training\n",
    "    while not done:\n",
    "        step_count += 1\n",
    "        x = np.reshape(state, [1, input_size])\n",
    "        # Choose an action by greedily (with e chance of random action) from\n",
    "        # the Q-network\n",
    "        Q = model.predict(x)\n",
    "        \n",
    "        if np.random.rand(1) < e:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q)\n",
    "\n",
    "        # Get new state and reward from environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            Q[0, action] = -100\n",
    "        else:\n",
    "            x_next = np.reshape(next_state, [1, input_size])\n",
    "            # Obtain the Q' values by feeding the new state through our network\n",
    "            Q_next =model.predict(x_next)\n",
    "            Q[0, action] = reward + dis * np.max(Q_next)\n",
    "\n",
    "        # Train our network using target and predicted Q values on each episode\n",
    "        model.fit(x,Q, verbose = 0)\n",
    "        state = next_state\n",
    "\n",
    "    step_history.append(step_count)\n",
    "    print(\"Episode: {}  steps: {}\".format(episode, step_count))\n",
    "    # If last 10's avg steps are 500, it's good enough\n",
    "    if len(step_history) > 10 and np.mean(step_history[-10:]) > 500:\n",
    "        break\n",
    "\n",
    "# See our trained network in action\n",
    "observation = env.reset()\n",
    "reward_sum = 0\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    x = np.reshape(observation, [1, input_size])\n",
    "    Q = model.predict(x)\n",
    "    action = np.argmax(Q)\n",
    "\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        print(\"Total score: {}\".format(reward_sum))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(step_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
